{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8208917,
          "sourceType": "datasetVersion",
          "datasetId": 4864451
        },
        {
          "sourceId": 8221216,
          "sourceType": "datasetVersion",
          "datasetId": 4873869
        }
      ],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Loading word vectors (elmo.py)"
      ],
      "metadata": {
        "id": "4-8-CxxxEmcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "\n",
        "word_vectors = api.load(\"word2vec-google-news-300\")\n",
        "word_vectors.save_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY1vjNAKFG5B",
        "outputId": "3d26b045-7f98-4e42-9728-d513af63a60d",
        "execution": {
          "iopub.status.busy": "2024-04-23T18:39:31.478600Z",
          "iopub.execute_input": "2024-04-23T18:39:31.479376Z",
          "iopub.status.idle": "2024-04-23T18:40:58.993447Z",
          "shell.execute_reply.started": "2024-04-23T18:39:31.479337Z",
          "shell.execute_reply": "2024-04-23T18:40:58.992590Z"
        },
        "trusted": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    words = text.split()\n",
        "    if words:\n",
        "        rand_idx = random.randint(0, len(words) - 1)\n",
        "        words[rand_idx] = \"unk\"\n",
        "    text = ' '.join(words)\n",
        "    return text"
      ],
      "metadata": {
        "id": "4A_cP_UaFyZm",
        "execution": {
          "iopub.status.busy": "2024-04-23T18:41:16.642186Z",
          "iopub.execute_input": "2024-04-23T18:41:16.642890Z",
          "iopub.status.idle": "2024-04-23T18:41:16.755191Z",
          "shell.execute_reply.started": "2024-04-23T18:41:16.642861Z",
          "shell.execute_reply": "2024-04-23T18:41:16.754300Z"
        },
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weqei7sIGBw8",
        "outputId": "0aa71ebe-48e6-4642-a8fa-9e6b23b21bff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"drive/MyDrive/NLP_A4/train.csv\"\n",
        "file1 = open(file_path, \"r\")\n",
        "data = csv.reader(file1)"
      ],
      "metadata": {
        "id": "Q_vJbDNAHd2H",
        "execution": {
          "iopub.status.busy": "2024-04-23T18:43:00.749969Z",
          "iopub.execute_input": "2024-04-23T18:43:00.750327Z",
          "iopub.status.idle": "2024-04-23T18:43:00.756534Z",
          "shell.execute_reply.started": "2024-04-23T18:43:00.750299Z",
          "shell.execute_reply": "2024-04-23T18:43:00.755415Z"
        },
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7Ek4nN5Ir2r",
        "outputId": "d4b7daf5-2490-4404-863a-e047f6ae0db8",
        "execution": {
          "iopub.status.busy": "2024-04-23T18:43:01.574130Z",
          "iopub.execute_input": "2024-04-23T18:43:01.574484Z",
          "iopub.status.idle": "2024-04-23T18:43:01.580785Z",
          "shell.execute_reply.started": "2024-04-23T18:43:01.574457Z",
          "shell.execute_reply": "2024-04-23T18:43:01.579870Z"
        },
        "trusted": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Class Index', 'Description']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_sentences = 20000\n",
        "\n",
        "with open(file_path, \"r\") as file1:\n",
        "    data = csv.reader(file1)\n",
        "    corp_list = []\n",
        "    corp_str = \"sos \"\n",
        "    for i, row in enumerate(data):\n",
        "        if i >= max_sentences:\n",
        "            break\n",
        "        string = preprocess(row[1])\n",
        "        corp_list.append(string)\n",
        "        corp_str += string + \" sos eos \"\n",
        "\n",
        "final_list = []\n",
        "corp_words = corp_str.split()\n",
        "# make sentences of 32 words\n",
        "for i in range(0, len(corp_words), 32):\n",
        "    final_list.append(corp_words[i:i+32])\n",
        "print(len(final_list))\n",
        "\n",
        "\n",
        "word2idx = {}\n",
        "idx2word = {}\n",
        "word_set = set()\n",
        "for i, word in enumerate(corp_words):\n",
        "    word_set.add(word)\n",
        "i = 0\n",
        "for word in word_set:\n",
        "    if word not in word2idx.keys():\n",
        "        word2idx[word] = i\n",
        "        idx2word[i] = word\n",
        "        i+=1\n",
        "\n",
        "from torch import nn, optim\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Making Dataset...\")\n",
        "class dual_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sent_list, word2idx, idx2word):\n",
        "        self.sent_list = sent_list\n",
        "        self.word2idx = word2idx\n",
        "        self.idx2word = idx2word\n",
        "        self.forward_labels, self.forward_targets = self.forward()\n",
        "        self.backward_labels, self.backward_targets = self.backward()\n",
        "    def forward(self):\n",
        "        labels = []\n",
        "        targets = []\n",
        "        for sent in self.sent_list:\n",
        "            loc_labels = []\n",
        "            loc_targets = []\n",
        "            for i in range(1, len(sent)-1):\n",
        "                loc_labels.append(self.word2idx[sent[i]])\n",
        "                loc_targets.append(self.word2idx[sent[i+1]])\n",
        "            labels.append(loc_labels)\n",
        "            targets.append(loc_targets)\n",
        "        return labels, targets\n",
        "    def backward(self):\n",
        "        labels = []\n",
        "        targets = []\n",
        "        for_labels = self.forward_labels\n",
        "        for_targets = self.forward_targets\n",
        "        for label in for_labels:\n",
        "            rev_label = label[::-1]\n",
        "            labels.append(rev_label)\n",
        "        for target in for_targets:\n",
        "            rev_target = target[::-1]\n",
        "            targets.append(rev_target)\n",
        "        return labels, targets\n",
        "    def __len__(self):\n",
        "        return len(self.sent_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if len(self.forward_labels[idx]) != len(self.forward_labels[0]):\n",
        "            idx = idx -1\n",
        "        forward_labels = torch.LongTensor(self.forward_labels[idx])\n",
        "        backward_labels = torch.LongTensor(self.backward_labels[idx])\n",
        "        forward_targets = torch.LongTensor(self.forward_targets[idx])\n",
        "        backward_targets = torch.LongTensor(self.backward_targets[idx])\n",
        "        return forward_labels, backward_labels, forward_targets, backward_targets\n",
        "\n",
        "my_dataset = dual_dataset(final_list, word2idx, idx2word)\n",
        "class lstm(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(lstm, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        # lstm with 2 stacks\n",
        "        num_stacks = 2\n",
        "        self.embeddings = nn.Embedding(vocab_size, 300)\n",
        "        self.lstm = nn.LSTM(300, 300, 1, batch_first=True)\n",
        "        self.lstm1 = nn.LSTM(300, 300, 1, batch_first=True)\n",
        "        self.linear = nn.Linear(300, vocab_size)\n",
        "    def forward(self, x):\n",
        "        embed = self.embeddings(x)\n",
        "        x1, _ = self.lstm(embed)\n",
        "        x2, _ = self.lstm1(x1)\n",
        "        x = self.linear(x2)\n",
        "        return x, (embed, x1, x2)\n",
        "\n",
        "forward_model = lstm(len(word2idx)).to(device)\n",
        "backward_model = lstm(len(word2idx)).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer1 = optim.Adam(forward_model.parameters(), lr=0.001)\n",
        "optimizer2 = optim.Adam(backward_model.parameters(), lr=0.001)\n",
        "dataloader = torch.utils.data.DataLoader(my_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "torch.save(word2idx, \"word2idx.pt\")\n",
        "torch.save(idx2word, \"idx2word.pt\")\n",
        "print(\"Done making the dictionaries\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPhIkZOAEonX",
        "outputId": "7b6fae65-9cd3-442f-9a82-e1a8c6d0c638",
        "execution": {
          "iopub.status.busy": "2024-04-23T18:43:02.725790Z",
          "iopub.execute_input": "2024-04-23T18:43:02.726626Z",
          "iopub.status.idle": "2024-04-23T18:43:11.886606Z",
          "shell.execute_reply.started": "2024-04-23T18:43:02.726589Z",
          "shell.execute_reply": "2024-04-23T18:43:11.885595Z"
        },
        "trusted": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20452\n",
            "Making Dataset...\n",
            "Done making the dictionaries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 15"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T18:43:46.768909Z",
          "iopub.execute_input": "2024-04-23T18:43:46.769275Z",
          "iopub.status.idle": "2024-04-23T18:43:46.773261Z",
          "shell.execute_reply.started": "2024-04-23T18:43:46.769249Z",
          "shell.execute_reply": "2024-04-23T18:43:46.772348Z"
        },
        "trusted": true,
        "id": "NdzpSfmU0-Kc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "for epoch in range(n_epochs):\n",
        "    forward_model.train()\n",
        "    backward_model.train()\n",
        "    total_forward_loss = 0.0\n",
        "    total_backward_loss = 0.0\n",
        "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Epoch {}\".format(epoch))\n",
        "    for batch, (forward_labels, backward_labels, forward_targets, backward_targets) in progress_bar:\n",
        "        forward_targets = forward_targets.to(device)\n",
        "        backward_targets = backward_targets.to(device)\n",
        "        forward_labels = forward_labels.to(device)\n",
        "        backward_labels = backward_labels.to(device)\n",
        "        # change the shape of forward_vcts and backward_vcts to batch-size, seq_len, embedding_dim\n",
        "        forward_targets = forward_targets.view(forward_targets.shape[1], forward_targets.shape[0])\n",
        "        backward_targets = backward_targets.view(backward_targets.shape[1], backward_targets.shape[0])\n",
        "        forward_labels = forward_labels.view(forward_labels.shape[1], forward_labels.shape[0])\n",
        "        backward_labels = backward_labels.view(backward_labels.shape[1], backward_labels.shape[0])\n",
        "        # forward pass\n",
        "        optimizer1.zero_grad()\n",
        "        forward_out,_  = forward_model(forward_labels)\n",
        "        forward_out = forward_out.view(forward_out.shape[0]*forward_out.shape[1], forward_out.shape[2])\n",
        "        forward_targets = forward_targets.view(forward_targets.shape[0]*forward_targets.shape[1])\n",
        "        forward_loss = loss_fn(forward_out, forward_targets)\n",
        "        forward_loss.backward()\n",
        "        optimizer1.step()\n",
        "        total_forward_loss += forward_loss.item()\n",
        "        # backward pass\n",
        "        optimizer2.zero_grad()\n",
        "        backward_out,_ = backward_model(backward_labels)\n",
        "        backward_out = backward_out.view(backward_out.shape[0]*backward_out.shape[1], backward_out.shape[2])\n",
        "        backward_targets = backward_targets.view(backward_targets.shape[0]*backward_targets.shape[1])\n",
        "        backward_loss = loss_fn(backward_out, backward_targets)\n",
        "        backward_loss.backward()\n",
        "        optimizer2.step()\n",
        "        total_backward_loss += backward_loss.item()\n",
        "        forward_peplexity = torch.exp(forward_loss)\n",
        "        backward_peplexity = torch.exp(backward_loss)\n",
        "        progress_bar.set_postfix(forward_loss=forward_loss.item(), backward_loss=backward_loss.item(), avg_forward_loss=total_forward_loss/(batch+1), avg_backward_loss=total_backward_loss/(batch+1))\n",
        "\n",
        "    # Calculate average losses\n",
        "    average_forward_loss = total_forward_loss / len(dataloader)\n",
        "    average_backward_loss = total_backward_loss / len(dataloader)\n",
        "\n",
        "    # Print epoch-level loss\n",
        "    print(\"Epoch: {}, Average Forward Loss: {:.4f}, Average Backward Loss: {:.4f}\".format(epoch, average_forward_loss, average_backward_loss))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EZUhWKfRqD6",
        "outputId": "e996a7d4-6137-4eef-fc7f-07919ed4bf5b",
        "execution": {
          "iopub.status.busy": "2024-04-23T18:44:45.608641Z",
          "iopub.execute_input": "2024-04-23T18:44:45.609521Z",
          "iopub.status.idle": "2024-04-23T20:23:26.842259Z",
          "shell.execute_reply.started": "2024-04-23T18:44:45.609484Z",
          "shell.execute_reply": "2024-04-23T20:23:26.841504Z"
        },
        "trusted": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 410/410 [00:40<00:00, 10.12it/s, avg_backward_loss=6.02, avg_forward_loss=7.43, backward_loss=5.48, forward_loss=6.89]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Average Forward Loss: 7.4291, Average Backward Loss: 6.0187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 410/410 [00:42<00:00,  9.72it/s, avg_backward_loss=3.4, avg_forward_loss=6.85, backward_loss=4.54, forward_loss=6.52]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Average Forward Loss: 6.8535, Average Backward Loss: 3.4019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 410/410 [00:41<00:00,  9.76it/s, avg_backward_loss=2.2, avg_forward_loss=6.5, backward_loss=4.56, forward_loss=6.79]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Average Forward Loss: 6.5022, Average Backward Loss: 2.2035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 410/410 [00:41<00:00,  9.88it/s, avg_backward_loss=1.62, avg_forward_loss=6.22, backward_loss=3.58, forward_loss=5.72]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Average Forward Loss: 6.2177, Average Backward Loss: 1.6156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 410/410 [00:41<00:00,  9.85it/s, avg_backward_loss=1.28, avg_forward_loss=5.98, backward_loss=3.53, forward_loss=5.48]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Average Forward Loss: 5.9804, Average Backward Loss: 1.2766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 410/410 [00:41<00:00,  9.87it/s, avg_backward_loss=1.05, avg_forward_loss=5.78, backward_loss=3.33, forward_loss=5.43]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Average Forward Loss: 5.7770, Average Backward Loss: 1.0503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 410/410 [00:41<00:00,  9.87it/s, avg_backward_loss=0.886, avg_forward_loss=5.6, backward_loss=3.51, forward_loss=5.71]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6, Average Forward Loss: 5.5977, Average Backward Loss: 0.8861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 410/410 [00:41<00:00,  9.86it/s, avg_backward_loss=0.762, avg_forward_loss=5.43, backward_loss=3.22, forward_loss=5.84]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7, Average Forward Loss: 5.4332, Average Backward Loss: 0.7619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 410/410 [00:41<00:00,  9.86it/s, avg_backward_loss=0.666, avg_forward_loss=5.27, backward_loss=3.25, forward_loss=5.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8, Average Forward Loss: 5.2732, Average Backward Loss: 0.6663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 410/410 [00:41<00:00,  9.84it/s, avg_backward_loss=0.591, avg_forward_loss=5.11, backward_loss=2.84, forward_loss=5.09]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9, Average Forward Loss: 5.1118, Average Backward Loss: 0.5914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 410/410 [00:41<00:00,  9.82it/s, avg_backward_loss=0.538, avg_forward_loss=4.95, backward_loss=3.78, forward_loss=5.83]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10, Average Forward Loss: 4.9540, Average Backward Loss: 0.5377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 410/410 [00:41<00:00,  9.83it/s, avg_backward_loss=0.496, avg_forward_loss=4.8, backward_loss=3.31, forward_loss=5.78]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11, Average Forward Loss: 4.7997, Average Backward Loss: 0.4961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 410/410 [00:41<00:00,  9.82it/s, avg_backward_loss=0.466, avg_forward_loss=4.65, backward_loss=3.48, forward_loss=5.76]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12, Average Forward Loss: 4.6507, Average Backward Loss: 0.4661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 410/410 [00:41<00:00,  9.78it/s, avg_backward_loss=0.446, avg_forward_loss=4.51, backward_loss=3.13, forward_loss=4.98]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13, Average Forward Loss: 4.5061, Average Backward Loss: 0.4464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 410/410 [00:41<00:00,  9.81it/s, avg_backward_loss=0.432, avg_forward_loss=4.37, backward_loss=3.21, forward_loss=5.02]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14, Average Forward Loss: 4.3719, Average Backward Loss: 0.4317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(forward_model, f\"forward_model_final.pt\")\n",
        "torch.save(backward_model, f\"backward_model_final.pt\")"
      ],
      "metadata": {
        "id": "MARIWk8hRyev",
        "execution": {
          "iopub.status.busy": "2024-04-23T20:52:31.081194Z",
          "iopub.execute_input": "2024-04-23T20:52:31.082006Z",
          "iopub.status.idle": "2024-04-23T20:52:32.144380Z",
          "shell.execute_reply.started": "2024-04-23T20:52:31.081973Z",
          "shell.execute_reply": "2024-04-23T20:52:32.143315Z"
        },
        "trusted": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Downstream classification task (classification.py)"
      ],
      "metadata": {
        "id": "rhYMNwfS0-Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "import os\n",
        "\n",
        "from torch import nn, optim\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import regex as re\n",
        "print(\"Done importing...\")\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'[0-9]+', '', text)\n",
        "    text = text.strip()\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "class lstm(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(lstm, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        # lstm with 2 stacks\n",
        "        num_stacks = 2\n",
        "        self.embeddings = nn.Embedding(vocab_size, 300)\n",
        "        self.lstm = nn.LSTM(300, 300, 1, batch_first=True)\n",
        "        self.lstm1 = nn.LSTM(300, 300, 1, batch_first=True)\n",
        "        self.linear = nn.Linear(300, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed = self.embeddings(x)\n",
        "        x1, _ = self.lstm(embed)\n",
        "        x2, _ = self.lstm1(x1)\n",
        "        x = self.linear(x2)\n",
        "        return x, (embed, x1, x2)\n",
        "\n",
        "def make_dataset(data, word2idx, idx2word, max_sentences=20000):\n",
        "    seq_list = []\n",
        "    label_list = []\n",
        "    length_list = []\n",
        "    data = data[1:max_sentences+1]  # Select only the first max_sentences\n",
        "    for item in data:\n",
        "        idx_seq = []\n",
        "        item = item.strip().split(',')  # Split the line into comma-separated values\n",
        "        label = item[0]\n",
        "        seq = preprocess(item[1])\n",
        "        seq = seq.split()\n",
        "        for word in seq:\n",
        "            if word in word2idx.keys():\n",
        "                idx_seq.append(word2idx[word])\n",
        "            else:\n",
        "                idx_seq.append(word2idx[\"unk\"])\n",
        "        length = len(idx_seq)\n",
        "        length_list.append(length)\n",
        "        seq_list.append(idx_seq)\n",
        "        label_list.append(label)\n",
        "    return seq_list, label_list, length_list\n",
        "\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "word2idx_path = \"word2idx.pt\"\n",
        "idx2word_path = \"idx2word.pt\"\n",
        "\n",
        "word2idx = torch.load(word2idx_path)\n",
        "idx2word = torch.load(idx2word_path)\n",
        "\n",
        "def load_dataset(filename):\n",
        "    file_path = os.path.join(\"drive/MyDrive/NLP_A4\", filename)  # Update the dataset folder path\n",
        "    with open(file_path, \"r\") as file:\n",
        "        data = file.readlines()\n",
        "    return data\n",
        "\n",
        "# Load train and test datasets\n",
        "train_data = load_dataset(\"train.csv\")\n",
        "test_data = load_dataset(\"test.csv\")\n",
        "\n",
        "# Preprocess and create sequences, labels, and lengths\n",
        "train_seqs, train_labels, train_lengths = make_dataset(train_data, word2idx, idx2word)\n",
        "test_seqs, test_labels, test_lengths = make_dataset(test_data, word2idx, idx2word)\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "train_seqs = pad_sequence([torch.LongTensor(i) for i in train_seqs], batch_first=True)\n",
        "test_seqs = pad_sequence([torch.LongTensor(i) for i in test_seqs], batch_first=True)\n",
        "\n",
        "train_labels = torch.LongTensor([int(i) for i in train_labels])\n",
        "test_labels = torch.LongTensor([int(i) for i in test_labels])\n",
        "train_lengths = torch.LongTensor(train_lengths)\n",
        "test_lengths = torch.LongTensor(test_lengths)\n",
        "\n",
        "print(\"train, test input shapes: \", train_seqs.shape, test_seqs.shape)\n",
        "print(\"train, test label shapes: \", train_labels.shape, test_labels.shape)\n",
        "print(\"train, test length shapes: \", train_lengths.shape, test_lengths.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, num_classes, in_dim):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.linear_layer = nn.Linear(in_dim, 600)\n",
        "        self.lstm = nn.LSTM(600, 300, 1, batch_first=True)\n",
        "        self.linear_layer2 = nn.Linear(300, num_classes)\n",
        "\n",
        "        # Initialize learnable lambda parameters\n",
        "        self.lambda_0 = nn.Parameter(torch.rand(1))\n",
        "        self.lambda_1 = nn.Parameter(torch.rand(1))\n",
        "        self.lambda_2 = nn.Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, x0, x1, x2, length):\n",
        "        x = self.linear_layer(self.lambda_0 * x0 + self.lambda_1 * x1 + self.lambda_2 * x2)\n",
        "        x, _ = self.lstm(x)\n",
        "\n",
        "        final_preds = []\n",
        "        loc_cnt = 0\n",
        "        for loc_len in length:\n",
        "            loc_x = x[loc_cnt, loc_len-1, :]\n",
        "            loc_out = self.linear_layer2(loc_x)\n",
        "            loc_cnt += 1\n",
        "            final_preds.append(loc_out)\n",
        "        final_preds = torch.stack(final_preds)\n",
        "        return final_preds\n",
        "\n",
        "forward_lstm = torch.load(\"backward_model_final.pt\").to(device)\n",
        "backward_lstm = torch.load(\"forward_model_final.pt\").to(device)\n",
        "\n",
        "# Freeze the ELMo model parameters\n",
        "for param in list(forward_lstm.parameters()) + list(backward_lstm.parameters()):\n",
        "    param.requires_grad = False\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_seqs, train_labels, train_lengths)\n",
        "\n",
        "# split train set into train and validation set 20 percent\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(test_seqs, test_labels, test_lengths)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=50, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "in_num = 600\n",
        "num_classes = 5\n",
        "downstream_model = Classifier(num_classes, in_num).to(device)\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(downstream_model.parameters(), lr=0.0001)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "n_epochs = 15\n",
        "best_val_f1 = 0.0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_accurs = []\n",
        "    train_f1s = []\n",
        "    val_accurs = []\n",
        "    val_f1s = []\n",
        "\n",
        "    print(\"Epoch {}:\".format(epoch + 1))\n",
        "\n",
        "    # Training\n",
        "    downstream_model.train()\n",
        "    train_loader_tqdm = tqdm(train_loader, desc=\"Training\")\n",
        "    for data, label, length in train_loader_tqdm:\n",
        "        data = data.to(device)\n",
        "        data_reverse = data.flip(1)\n",
        "        _, (xf0, xf1, xf2) = forward_lstm(data)\n",
        "        _, (xb0, xb1, xb2) = backward_lstm(data_reverse)\n",
        "        xf0, xf1, xf2 = xf0.detach(), xf1.detach(), xf2.detach()\n",
        "        xb0, xb1, xb2 = xb0.detach(), xb1.detach(), xb2.detach()\n",
        "        x0 = torch.cat((xf0, xb0), dim=2)\n",
        "        x1 = torch.cat((xf1, xb1), dim=2)\n",
        "        x2 = torch.cat((xf2, xb2), dim=2)\n",
        "        output = downstream_model(x0, x1, x2, length)\n",
        "        loss = loss_func(output, label.to(device))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        accuracy = accuracy_score(label.cpu(), predicted.cpu())\n",
        "        micro_f1 = f1_score(label.cpu(), predicted.cpu(), average=\"micro\")\n",
        "        train_accurs.append(accuracy)\n",
        "        train_f1s.append(micro_f1)\n",
        "        train_loader_tqdm.set_postfix(loss=loss.item(), accuracy=accuracy, micro_f1=micro_f1)\n",
        "\n",
        "    train_accuracy = sum(train_accurs) / len(train_accurs)\n",
        "    train_micro_f1 = sum(train_f1s) / len(train_f1s)\n",
        "    print(\"Training - Accuracy: {:.4f}, Micro F1: {:.4f}\".format(train_accuracy, train_micro_f1))\n",
        "\n",
        "    # Validation\n",
        "    downstream_model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loader_tqdm = tqdm(val_loader, desc=\"Validation\")\n",
        "        for data, label, length in val_loader_tqdm:\n",
        "            data = data.to(device)\n",
        "            data_reverse = data.flip(1)\n",
        "            _, (xf0, xf1, xf2) = forward_lstm(data)\n",
        "            _, (xb0, xb1, xb2) = backward_lstm(data_reverse)\n",
        "            xf0, xf1, xf2 = xf0.detach(), xf1.detach(), xf2.detach()\n",
        "            xb0, xb1, xb2 = xb0.detach(), xb1.detach(), xb2.detach()\n",
        "            x0 = torch.cat((xf0, xb0), dim=2)\n",
        "            x1 = torch.cat((xf1, xb1), dim=2)\n",
        "            x2 = torch.cat((xf2, xb2), dim=2)\n",
        "            output = downstream_model(x0, x1, x2, length)\n",
        "\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            accuracy = accuracy_score(label.cpu(), predicted.cpu())\n",
        "            micro_f1 = f1_score(label.cpu(), predicted.cpu(), average=\"micro\")\n",
        "            val_accurs.append(accuracy)\n",
        "            val_f1s.append(micro_f1)\n",
        "            val_loader_tqdm.set_postfix(accuracy=accuracy, micro_f1=micro_f1)\n",
        "\n",
        "    val_accuracy = sum(val_accurs) / len(val_accurs)\n",
        "    val_micro_f1 = sum(val_f1s) / len(val_f1s)\n",
        "    print(\"Validation - Accuracy: {:.4f}, Micro F1: {:.4f}\".format(val_accuracy, val_micro_f1))\n",
        "\n",
        "    if val_micro_f1 > best_val_f1:\n",
        "        best_val_f1 = val_micro_f1\n",
        "        # Save the model\n",
        "        torch.save(downstream_model, \"best_downstream_model.pt\")\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Testing the best model\n",
        "best_downstream_model = torch.load(\"best_downstream_model.pt\")\n",
        "best_downstream_model.eval()\n",
        "\n",
        "test_accurs = []\n",
        "test_f1s = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, label, length in test_loader:\n",
        "        data = data.to(device)\n",
        "        data_reverse = data.flip(1)\n",
        "        _, (xf0, xf1, xf2) = forward_lstm(data)\n",
        "        _, (xb0, xb1, xb2) = backward_lstm(data_reverse)\n",
        "        xf0, xf1, xf2 = xf0.detach(), xf1.detach(), xf2.detach()\n",
        "        xb0, xb1, xb2 = xb0.detach(), xb1.detach(), xb2.detach()\n",
        "        x0 = torch.cat((xf0, xb0), dim=2)\n",
        "        x1 = torch.cat((xf1, xb1), dim=2)\n",
        "        x2 = torch.cat((xf2, xb2), dim=2)\n",
        "        output = best_downstream_model(x0, x1, x2, length)\n",
        "\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        accuracy = accuracy_score(label.cpu(), predicted.cpu())\n",
        "        micro_f1 = f1_score(label.cpu(), predicted.cpu(), average=\"micro\")\n",
        "        test_accurs.append(accuracy)\n",
        "        test_f1s.append(micro_f1)\n",
        "\n",
        "test_accuracy = sum(test_accurs) / len(test_accurs)\n",
        "test_micro_f1 = sum(test_f1s) / len(test_f1s)\n",
        "print(\"Test - Accuracy: {:.4f}, Micro F1: {:.4f}\".format(test_accuracy, test_micro_f1))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-25T05:28:53.631533Z",
          "iopub.execute_input": "2024-04-25T05:28:53.631936Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvsC5KRz0-Kd",
        "outputId": "166059a9-aaed-4bd8-e84c-e8df611aaaee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done importing...\n",
            "train, test input shapes:  torch.Size([20000, 117]) torch.Size([7600, 104])\n",
            "train, test label shapes:  torch.Size([20000]) torch.Size([7600])\n",
            "train, test length shapes:  torch.Size([20000]) torch.Size([7600])\n",
            "Epoch 1:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [01:01<00:00,  5.20it/s, accuracy=0.5, loss=1.25, micro_f1=0.5]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training - Accuracy: 0.3551, Micro F1: 0.3551\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.45it/s, accuracy=0.36, micro_f1=0.36]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation - Accuracy: 0.4108, Micro F1: 0.4107\n",
            "Epoch 2:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [01:00<00:00,  5.33it/s, accuracy=0.3, loss=1.23, micro_f1=0.3]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training - Accuracy: 0.4296, Micro F1: 0.4296\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.37it/s, accuracy=0.46, micro_f1=0.46]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation - Accuracy: 0.4678, Micro F1: 0.4678\n",
            "Epoch 3:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.49it/s, accuracy=0.38, loss=1.2, micro_f1=0.38]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training - Accuracy: 0.4910, Micro F1: 0.4910\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.38it/s, accuracy=0.52, micro_f1=0.52]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation - Accuracy: 0.5065, Micro F1: 0.5065\n",
            "Epoch 4:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.48it/s, accuracy=0.7, loss=0.85, micro_f1=0.7]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training - Accuracy: 0.5404, Micro F1: 0.5404\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.41it/s, accuracy=0.52, micro_f1=0.52]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation - Accuracy: 0.5455, Micro F1: 0.5455\n",
            "Epoch 5:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.49it/s, accuracy=0.6, loss=0.93, micro_f1=0.6]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training - Accuracy: 0.5767, Micro F1: 0.5767\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.40it/s, accuracy=0.7, micro_f1=0.7]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation - Accuracy: 0.5827, Micro F1: 0.5827\n",
            "Epoch 6:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.50it/s, accuracy=0.6, loss=0.953, micro_f1=0.6]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training - Accuracy: 0.5997, Micro F1: 0.5997\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.37it/s, accuracy=0.62, micro_f1=0.62]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation - Accuracy: 0.5885, Micro F1: 0.5885\n",
            "Epoch 7:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.51it/s, accuracy=0.52, loss=1, micro_f1=0.52]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training - Accuracy: 0.6216, Micro F1: 0.6216\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.37it/s, accuracy=0.54, micro_f1=0.54]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation - Accuracy: 0.6015, Micro F1: 0.6015\n",
            "Epoch 8:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.49it/s, accuracy=0.64, loss=0.88, micro_f1=0.64]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training - Accuracy: 0.6387, Micro F1: 0.6387\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.45it/s, accuracy=0.48, micro_f1=0.48]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation - Accuracy: 0.6095, Micro F1: 0.6095\n",
            "Epoch 9:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.49it/s, accuracy=0.6, loss=0.854, micro_f1=0.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.6602, Micro F1: 0.6602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.35it/s, accuracy=0.54, micro_f1=0.54]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6222, Micro F1: 0.6222\n",
            "Epoch 10:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.51it/s, accuracy=0.72, loss=0.693, micro_f1=0.72]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.6684, Micro F1: 0.6684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.41it/s, accuracy=0.56, micro_f1=0.56]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6182, Micro F1: 0.6182\n",
            "Epoch 11:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.48it/s, accuracy=0.8, loss=0.618, micro_f1=0.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.6859, Micro F1: 0.6859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.39it/s, accuracy=0.62, micro_f1=0.62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6385, Micro F1: 0.6385\n",
            "Epoch 12:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.51it/s, accuracy=0.72, loss=0.719, micro_f1=0.72]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.7062, Micro F1: 0.7062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.35it/s, accuracy=0.62, micro_f1=0.62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6400, Micro F1: 0.6400\n",
            "Epoch 13:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.47it/s, accuracy=0.64, loss=0.851, micro_f1=0.64]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.7154, Micro F1: 0.7154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.42it/s, accuracy=0.68, micro_f1=0.68]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6570, Micro F1: 0.6570\n",
            "Epoch 14:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.49it/s, accuracy=0.78, loss=0.636, micro_f1=0.78]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.7352, Micro F1: 0.7352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.40it/s, accuracy=0.76, micro_f1=0.76]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6660, Micro F1: 0.6660\n",
            "Epoch 15:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.50it/s, accuracy=0.74, loss=0.624, micro_f1=0.74]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.7471, Micro F1: 0.7471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.40it/s, accuracy=0.82, micro_f1=0.82]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6660, Micro F1: 0.6660\n",
            "Training complete.\n",
            "Test - Accuracy: 0.6150, Micro F1: 0.6150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameter Tuning##"
      ],
      "metadata": {
        "id": "ZrOjea6GB_va"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainable λs"
      ],
      "metadata": {
        "id": "EoeTBBGE0-Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "import gensim\n",
        "import os\n",
        "\n",
        "from torch import nn, optim\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import regex as re\n",
        "print(\"Done importing...\")\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'[0-9]+', '', text)\n",
        "    text = text.strip()\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "class lstm(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(lstm, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        # lstm with 2 stacks\n",
        "        num_stacks = 2\n",
        "        self.embeddings = nn.Embedding(vocab_size, 300)\n",
        "        self.lstm = nn.LSTM(300, 300, 1, batch_first=True)\n",
        "        self.lstm1 = nn.LSTM(300, 300, 1, batch_first=True)\n",
        "        self.linear = nn.Linear(300, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed = self.embeddings(x)\n",
        "        x1, _ = self.lstm(embed)\n",
        "        x2, _ = self.lstm1(x1)\n",
        "        x = self.linear(x2)\n",
        "        return x, (embed, x1, x2)\n",
        "\n",
        "def make_dataset(data, word2idx, idx2word, max_sentences=20000):\n",
        "    seq_list = []\n",
        "    label_list = []\n",
        "    length_list = []\n",
        "    data = data[1:max_sentences+1]  # Select only the first max_sentences\n",
        "    for item in data:\n",
        "        idx_seq = []\n",
        "        item = item.strip().split(',')\n",
        "        label = item[0]\n",
        "        seq = preprocess(item[1])\n",
        "        seq = seq.split()\n",
        "        for word in seq:\n",
        "            if word in word2idx.keys():\n",
        "                idx_seq.append(word2idx[word])\n",
        "            else:\n",
        "                idx_seq.append(word2idx[\"unk\"])\n",
        "        length = len(idx_seq)\n",
        "        length_list.append(length)\n",
        "        seq_list.append(idx_seq)\n",
        "        label_list.append(label)\n",
        "    return seq_list, label_list, length_list\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "word2idx_path = \"word2idx.pt\"\n",
        "idx2word_path = \"idx2word.pt\"\n",
        "\n",
        "word2idx = torch.load(word2idx_path)\n",
        "idx2word = torch.load(idx2word_path)\n",
        "\n",
        "def load_dataset(filename):\n",
        "    file_path = os.path.join(\"drive/MyDrive/NLP_A4\", filename)\n",
        "    with open(file_path, \"r\") as file:\n",
        "        data = file.readlines()\n",
        "    return data\n",
        "\n",
        "train_data = load_dataset(\"train.csv\")\n",
        "test_data = load_dataset(\"test.csv\")\n",
        "\n",
        "#preprocess and create sequences, labels, and lengths\n",
        "train_seqs, train_labels, train_lengths = make_dataset(train_data, word2idx, idx2word)\n",
        "test_seqs, test_labels, test_lengths = make_dataset(test_data, word2idx, idx2word)\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "train_seqs = pad_sequence([torch.LongTensor(i) for i in train_seqs], batch_first=True)\n",
        "test_seqs = pad_sequence([torch.LongTensor(i) for i in test_seqs], batch_first=True)\n",
        "\n",
        "train_labels = torch.LongTensor([int(i) for i in train_labels])\n",
        "test_labels = torch.LongTensor([int(i) for i in test_labels])\n",
        "train_lengths = torch.LongTensor(train_lengths)\n",
        "test_lengths = torch.LongTensor(test_lengths)\n",
        "\n",
        "print(\"train, test input shapes: \", train_seqs.shape, test_seqs.shape)\n",
        "print(\"train, test label shapes: \", train_labels.shape, test_labels.shape)\n",
        "print(\"train, test length shapes: \", train_lengths.shape, test_lengths.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, num_classes, in_dim):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.linear_layer = nn.Linear(in_dim, 600)\n",
        "        self.lstm = nn.LSTM(600, 300, 1, batch_first=True)\n",
        "        self.linear_layer2 = nn.Linear(300, num_classes)\n",
        "\n",
        "        # Initialize learnable lambda parameters\n",
        "        self.lambda_0 = nn.Parameter(torch.rand(1))\n",
        "        self.lambda_1 = nn.Parameter(torch.rand(1))\n",
        "        self.lambda_2 = nn.Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, x0, x1, x2, length):\n",
        "        x = self.linear_layer(self.lambda_0 * x0 + self.lambda_1 * x1 + self.lambda_2 * x2)\n",
        "        x, _ = self.lstm(x)\n",
        "\n",
        "        final_preds = []\n",
        "        loc_cnt = 0\n",
        "        for loc_len in length:\n",
        "            loc_x = x[loc_cnt, loc_len-1, :]\n",
        "            loc_out = self.linear_layer2(loc_x)\n",
        "            loc_cnt += 1\n",
        "            final_preds.append(loc_out)\n",
        "        final_preds = torch.stack(final_preds)\n",
        "        return final_preds\n",
        "\n",
        "forward_lstm = torch.load(\"backward_model_final.pt\").to(device)\n",
        "backward_lstm = torch.load(\"forward_model_final.pt\").to(device)\n",
        "\n",
        "for param in list(forward_lstm.parameters()) + list(backward_lstm.parameters()):\n",
        "    param.requires_grad = False\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_seqs, train_labels, train_lengths)\n",
        "\n",
        "# split train set into train and validation set 20 percent\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(test_seqs, test_labels, test_lengths)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=50, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "in_num = 600\n",
        "num_classes = 5\n",
        "downstream_model = Classifier(num_classes, in_num).to(device)\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(downstream_model.parameters(), lr=0.0001)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "n_epochs = 10\n",
        "\n",
        "best_val_f1 = 0.0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_accurs = []\n",
        "    train_f1s = []\n",
        "    val_accurs = []\n",
        "    val_f1s = []\n",
        "\n",
        "    print(\"Epoch {}:\".format(epoch + 1))\n",
        "\n",
        "    # Training\n",
        "    downstream_model.train()\n",
        "    train_loader_tqdm = tqdm(train_loader, desc=\"Training\")\n",
        "    for data, label, length in train_loader_tqdm:\n",
        "        data = data.to(device)\n",
        "        data_reverse = data.flip(1)\n",
        "        _, (xf0, xf1, xf2) = forward_lstm(data)\n",
        "        _, (xb0, xb1, xb2) = backward_lstm(data_reverse)\n",
        "        xf0, xf1, xf2 = xf0.detach(), xf1.detach(), xf2.detach()\n",
        "        xb0, xb1, xb2 = xb0.detach(), xb1.detach(), xb2.detach()\n",
        "        x0 = torch.cat((xf0, xb0), dim=2)\n",
        "        x1 = torch.cat((xf1, xb1), dim=2)\n",
        "        x2 = torch.cat((xf2, xb2), dim=2)\n",
        "        output = downstream_model(x0, x1, x2, length)\n",
        "        loss = loss_func(output, label.to(device))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        accuracy = accuracy_score(label.cpu(), predicted.cpu())\n",
        "        micro_f1 = f1_score(label.cpu(), predicted.cpu(), average=\"micro\")\n",
        "        train_accurs.append(accuracy)\n",
        "        train_f1s.append(micro_f1)\n",
        "        train_loader_tqdm.set_postfix(loss=loss.item(), accuracy=accuracy, micro_f1=micro_f1)\n",
        "\n",
        "    train_accuracy = sum(train_accurs) / len(train_accurs)\n",
        "    train_micro_f1 = sum(train_f1s) / len(train_f1s)\n",
        "    print(\"Training - Accuracy: {:.4f}, Micro F1: {:.4f}\".format(train_accuracy, train_micro_f1))\n",
        "\n",
        "    # Validation\n",
        "    downstream_model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loader_tqdm = tqdm(val_loader, desc=\"Validation\")\n",
        "        for data, label, length in val_loader_tqdm:\n",
        "            data = data.to(device)\n",
        "            data_reverse = data.flip(1)\n",
        "            _, (xf0, xf1, xf2) = forward_lstm(data)\n",
        "            _, (xb0, xb1, xb2) = backward_lstm(data_reverse)\n",
        "            xf0, xf1, xf2 = xf0.detach(), xf1.detach(), xf2.detach()\n",
        "            xb0, xb1, xb2 = xb0.detach(), xb1.detach(), xb2.detach()\n",
        "            x0 = torch.cat((xf0, xb0), dim=2)\n",
        "            x1 = torch.cat((xf1, xb1), dim=2)\n",
        "            x2 = torch.cat((xf2, xb2), dim=2)\n",
        "            output = downstream_model(x0, x1, x2, length)\n",
        "\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            accuracy = accuracy_score(label.cpu(), predicted.cpu())\n",
        "            micro_f1 = f1_score(label.cpu(), predicted.cpu(), average=\"micro\")\n",
        "            val_accurs.append(accuracy)\n",
        "            val_f1s.append(micro_f1)\n",
        "            val_loader_tqdm.set_postfix(accuracy=accuracy, micro_f1=micro_f1)\n",
        "\n",
        "    val_accuracy = sum(val_accurs) / len(val_accurs)\n",
        "    val_micro_f1 = sum(val_f1s) / len(val_f1s)\n",
        "    print(\"Validation - Accuracy: {:.4f}, Micro F1: {:.4f}\".format(val_accuracy, val_micro_f1))\n",
        "\n",
        "    # Check if this is the best validation F1 score so far\n",
        "    if val_micro_f1 > best_val_f1:\n",
        "        best_val_f1 = val_micro_f1\n",
        "        # Save the model\n",
        "        torch.save(downstream_model, \"best_downstream_model.pt\")\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "best_downstream_model = torch.load(\"best_downstream_model.pt\")\n",
        "best_downstream_model.eval()\n",
        "\n",
        "test_accurs = []\n",
        "test_f1s = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, label, length in test_loader:\n",
        "        data = data.to(device)\n",
        "        data_reverse = data.flip(1)\n",
        "        _, (xf0, xf1, xf2) = forward_lstm(data)\n",
        "        _, (xb0, xb1, xb2) = backward_lstm(data_reverse)\n",
        "        xf0, xf1, xf2 = xf0.detach(), xf1.detach(), xf2.detach()\n",
        "        xb0, xb1, xb2 = xb0.detach(), xb1.detach(), xb2.detach()\n",
        "        x0 = torch.cat((xf0, xb0), dim=2)\n",
        "        x1 = torch.cat((xf1, xb1), dim=2)\n",
        "        x2 = torch.cat((xf2, xb2), dim=2)\n",
        "        output = best_downstream_model(x0, x1, x2, length)\n",
        "\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        accuracy = accuracy_score(label.cpu(), predicted.cpu())\n",
        "        micro_f1 = f1_score(label.cpu(), predicted.cpu(), average=\"micro\")\n",
        "        test_accurs.append(accuracy)\n",
        "        test_f1s.append(micro_f1)\n",
        "\n",
        "test_accuracy = sum(test_accurs) / len(test_accurs)\n",
        "test_micro_f1 = sum(test_f1s) / len(test_f1s)\n",
        "print(\"Test - Accuracy: {:.4f}, Micro F1: {:.4f}\".format(test_accuracy, test_micro_f1))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-25T06:39:56.691925Z",
          "iopub.execute_input": "2024-04-25T06:39:56.692291Z",
          "iopub.status.idle": "2024-04-25T07:22:56.550976Z",
          "shell.execute_reply.started": "2024-04-25T06:39:56.692258Z",
          "shell.execute_reply": "2024-04-25T07:22:56.549947Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3npcW2w40-Kf",
        "outputId": "0877a209-c51e-47a5-9ea6-b57da799ae72"
      },
      "execution_count": 12,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done importing...\n",
            "train, test input shapes:  torch.Size([20000, 117]) torch.Size([7600, 104])\n",
            "train, test label shapes:  torch.Size([20000]) torch.Size([7600])\n",
            "train, test length shapes:  torch.Size([20000]) torch.Size([7600])\n",
            "Epoch 1:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [01:04<00:00,  4.93it/s, accuracy=0.42, loss=1.24, micro_f1=0.42]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training - Accuracy: 0.3716, Micro F1: 0.3716\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.17it/s, accuracy=0.32, micro_f1=0.32]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation - Accuracy: 0.3615, Micro F1: 0.3615\n",
            "Epoch 2:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 320/320 [01:01<00:00,  5.19it/s, accuracy=0.56, loss=1.12, micro_f1=0.56]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training - Accuracy: 0.4777, Micro F1: 0.4777\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.38it/s, accuracy=0.46, micro_f1=0.46]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation - Accuracy: 0.5268, Micro F1: 0.5268\n",
            "Epoch 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [01:08<00:00,  4.70it/s, accuracy=0.58, loss=0.965, micro_f1=0.58]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.5546, Micro F1: 0.5546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.40it/s, accuracy=0.62, micro_f1=0.62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.5670, Micro F1: 0.5670\n",
            "Epoch 4:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.51it/s, accuracy=0.64, loss=0.86, micro_f1=0.64]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.5950, Micro F1: 0.5950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.39it/s, accuracy=0.58, micro_f1=0.58]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6192, Micro F1: 0.6192\n",
            "Epoch 5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.50it/s, accuracy=0.68, loss=0.884, micro_f1=0.68]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.6315, Micro F1: 0.6315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.40it/s, accuracy=0.54, micro_f1=0.54]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.5737, Micro F1: 0.5737\n",
            "Epoch 6:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.50it/s, accuracy=0.58, loss=0.869, micro_f1=0.58]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.6474, Micro F1: 0.6474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.41it/s, accuracy=0.6, micro_f1=0.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6485, Micro F1: 0.6485\n",
            "Epoch 7:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.51it/s, accuracy=0.64, loss=0.789, micro_f1=0.64]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.6766, Micro F1: 0.6766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.40it/s, accuracy=0.6, micro_f1=0.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6482, Micro F1: 0.6482\n",
            "Epoch 8:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.50it/s, accuracy=0.82, loss=0.566, micro_f1=0.82]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.6951, Micro F1: 0.6951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.38it/s, accuracy=0.8, micro_f1=0.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6685, Micro F1: 0.6685\n",
            "Epoch 9:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.49it/s, accuracy=0.7, loss=0.713, micro_f1=0.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.7169, Micro F1: 0.7169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.40it/s, accuracy=0.62, micro_f1=0.62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6755, Micro F1: 0.6755\n",
            "Epoch 10:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:57<00:00,  5.52it/s, accuracy=0.66, loss=0.725, micro_f1=0.66]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.7298, Micro F1: 0.7298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.40it/s, accuracy=0.78, micro_f1=0.78]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6900, Micro F1: 0.6900\n",
            "Training complete.\n",
            "Test - Accuracy: 0.6204, Micro F1: 0.6204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#testing the best model\n",
        "best_downstream_model = torch.load(\"best_downstream_model.pt\")\n",
        "best_downstream_model.eval()\n",
        "\n",
        "test_accurs = []\n",
        "test_f1s = []\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, label, length in test_loader:\n",
        "        data = data.to(device)\n",
        "        data_reverse = data.flip(1)\n",
        "        _, (xf0, xf1, xf2) = forward_lstm(data)\n",
        "        _, (xb0, xb1, xb2) = backward_lstm(data_reverse)\n",
        "        xf0, xf1, xf2 = xf0.detach(), xf1.detach(), xf2.detach()\n",
        "        xb0, xb1, xb2 = xb0.detach(), xb1.detach(), xb2.detach()\n",
        "        x0 = torch.cat((xf0, xb0), dim=2)\n",
        "        x1 = torch.cat((xf1, xb1), dim=2)\n",
        "        x2 = torch.cat((xf2, xb2), dim=2)\n",
        "        output = best_downstream_model(x0, x1, x2, length)\n",
        "\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        true_labels.extend(label.cpu().numpy())\n",
        "        predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "        accuracy = accuracy_score(label.cpu(), predicted.cpu())\n",
        "        micro_f1 = f1_score(label.cpu(), predicted.cpu(), average=\"micro\")\n",
        "        test_accurs.append(accuracy)\n",
        "        test_f1s.append(micro_f1)\n",
        "\n",
        "test_accuracy = sum(test_accurs) / len(test_accurs)\n",
        "test_micro_f1 = sum(test_f1s) / len(test_f1s)\n",
        "print(\"Test - Accuracy: {:.4f}, Micro F1: {:.4f}\".format(test_accuracy, test_micro_f1))\n",
        "\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-25T07:56:16.330081Z",
          "iopub.execute_input": "2024-04-25T07:56:16.330939Z",
          "iopub.status.idle": "2024-04-25T07:56:34.890277Z",
          "shell.execute_reply.started": "2024-04-25T07:56:16.330895Z",
          "shell.execute_reply": "2024-04-25T07:56:34.889306Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZMuoURk0-Kg",
        "outputId": "8e0504b9-6a98-40d1-a776-c9c35a1f7f34"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test - Accuracy: 0.6204, Micro F1: 0.6204\n",
            "Confusion Matrix:\n",
            "[[1216  177  160  347]\n",
            " [ 276 1084  132  408]\n",
            " [ 189  101 1016  594]\n",
            " [ 206   97  198 1399]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frozen λs"
      ],
      "metadata": {
        "id": "vpXZiRRe0-Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "import gensim\n",
        "import os\n",
        "\n",
        "from torch import nn, optim\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import regex as re\n",
        "print(\"Done importing...\")\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'[0-9]+', '', text)\n",
        "    text = text.strip()\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "class lstm(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(lstm, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        # lstm with 2 stacks\n",
        "        num_stacks = 2\n",
        "        self.embeddings = nn.Embedding(vocab_size, 300)\n",
        "        self.lstm = nn.LSTM(300, 300, 1, batch_first=True)\n",
        "        self.lstm1 = nn.LSTM(300, 300, 1, batch_first=True)\n",
        "        self.linear = nn.Linear(300, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed = self.embeddings(x)\n",
        "        x1, _ = self.lstm(embed)\n",
        "        x2, _ = self.lstm1(x1)\n",
        "        x = self.linear(x2)\n",
        "        return x, (embed, x1, x2)\n",
        "\n",
        "def make_dataset(data, word2idx, idx2word, max_sentences=20000):\n",
        "    seq_list = []\n",
        "    label_list = []\n",
        "    length_list = []\n",
        "    data = data[1:max_sentences+1]\n",
        "    for item in data:\n",
        "        idx_seq = []\n",
        "        item = item.strip().split(',')\n",
        "        label = item[0]\n",
        "        seq = preprocess(item[1])\n",
        "        seq = seq.split()\n",
        "        for word in seq:\n",
        "            if word in word2idx.keys():\n",
        "                idx_seq.append(word2idx[word])\n",
        "            else:\n",
        "                idx_seq.append(word2idx[\"unk\"])\n",
        "        length = len(idx_seq)\n",
        "        length_list.append(length)\n",
        "        seq_list.append(idx_seq)\n",
        "        label_list.append(label)\n",
        "    return seq_list, label_list, length_list\n",
        "\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "word2idx_path = \"word2idx.pt\"\n",
        "idx2word_path = \"idx2word.pt\"\n",
        "\n",
        "word2idx = torch.load(word2idx_path)\n",
        "idx2word = torch.load(idx2word_path)\n",
        "\n",
        "def load_dataset(filename):\n",
        "    file_path = os.path.join(\"drive/MyDrive/NLP_A4\", filename)\n",
        "    with open(file_path, \"r\") as file:\n",
        "        data = file.readlines()\n",
        "    return data\n",
        "\n",
        "train_data = load_dataset(\"train.csv\")\n",
        "test_data = load_dataset(\"test.csv\")\n",
        "\n",
        "#preprocess and create sequences, labels, and lengths\n",
        "train_seqs, train_labels, train_lengths = make_dataset(train_data, word2idx, idx2word)\n",
        "test_seqs, test_labels, test_lengths = make_dataset(test_data, word2idx, idx2word)\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "train_seqs = pad_sequence([torch.LongTensor(i) for i in train_seqs], batch_first=True)\n",
        "test_seqs = pad_sequence([torch.LongTensor(i) for i in test_seqs], batch_first=True)\n",
        "\n",
        "train_labels = torch.LongTensor([int(i) for i in train_labels])\n",
        "test_labels = torch.LongTensor([int(i) for i in test_labels])\n",
        "train_lengths = torch.LongTensor(train_lengths)\n",
        "test_lengths = torch.LongTensor(test_lengths)\n",
        "\n",
        "print(\"train, test input shapes: \", train_seqs.shape, test_seqs.shape)\n",
        "print(\"train, test label shapes: \", train_labels.shape, test_labels.shape)\n",
        "print(\"train, test length shapes: \", train_lengths.shape, test_lengths.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, num_classes, in_dim):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.linear_layer = nn.Linear(in_dim, 600)\n",
        "        self.lstm = nn.LSTM(600, 300, 1, batch_first=True)\n",
        "        self.linear_layer2 = nn.Linear(300, num_classes)\n",
        "\n",
        "        # Initialize learnable lambda parameters\n",
        "        self.lambda_0 = nn.Parameter(torch.rand(1))\n",
        "        self.lambda_1 = nn.Parameter(torch.rand(1))\n",
        "        self.lambda_2 = nn.Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, x0, x1, x2, length):\n",
        "        x = self.linear_layer(self.lambda_0 * x0 + self.lambda_1 * x1 + self.lambda_2 * x2)\n",
        "        x, _ = self.lstm(x)\n",
        "\n",
        "        final_preds = []\n",
        "        loc_cnt = 0\n",
        "        for loc_len in length:\n",
        "            loc_x = x[loc_cnt, loc_len-1, :]\n",
        "            loc_out = self.linear_layer2(loc_x)\n",
        "            loc_cnt += 1\n",
        "            final_preds.append(loc_out)\n",
        "        final_preds = torch.stack(final_preds)\n",
        "        return final_preds\n",
        "\n",
        "forward_lstm = torch.load(\"backward_model_final.pt\").to(device)\n",
        "backward_lstm = torch.load(\"forward_model_final.pt\").to(device)\n",
        "\n",
        "# Freeze the ELMo model parameters\n",
        "for param in list(forward_lstm.parameters()) + list(backward_lstm.parameters()):\n",
        "    param.requires_grad = False\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_seqs, train_labels, train_lengths)\n",
        "\n",
        "# split train set into train and validation set 20 percent\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(test_seqs, test_labels, test_lengths)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=50, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "in_num = 600\n",
        "num_classes = 5\n",
        "downstream_model = Classifier(num_classes, in_num).to(device)\n",
        "\n",
        "# Freeze the lambda parameters for the frozen setting\n",
        "for param in [downstream_model.lambda_0, downstream_model.lambda_1, downstream_model.lambda_2]:\n",
        "    param.requires_grad = False\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(downstream_model.parameters(), lr=0.0001)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "n_epochs = 10\n",
        "\n",
        "best_val_f1 = 0.0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_accurs = []\n",
        "    train_f1s = []\n",
        "    val_accurs = []\n",
        "    val_f1s = []\n",
        "\n",
        "    print(\"Epoch {}:\".format(epoch + 1))\n",
        "\n",
        "    # Training\n",
        "    downstream_model.train()\n",
        "    train_loader_tqdm = tqdm(train_loader, desc=\"Training\")\n",
        "    for data, label, length in train_loader_tqdm:\n",
        "        data = data.to(device)\n",
        "        data_reverse = data.flip(1)\n",
        "        _, (xf0, xf1, xf2) = forward_lstm(data)\n",
        "        _, (xb0, xb1, xb2) = backward_lstm(data_reverse)\n",
        "        xf0, xf1, xf2 = xf0.detach(), xf1.detach(), xf2.detach()\n",
        "        xb0, xb1, xb2 = xb0.detach(), xb1.detach(), xb2.detach()\n",
        "        x0 = torch.cat((xf0, xb0), dim=2)\n",
        "        x1 = torch.cat((xf1, xb1), dim=2)\n",
        "        x2 = torch.cat((xf2, xb2), dim=2)\n",
        "        output = downstream_model(x0, x1, x2, length)\n",
        "        loss = loss_func(output, label.to(device))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        accuracy = accuracy_score(label.cpu(), predicted.cpu())\n",
        "        micro_f1 = f1_score(label.cpu(), predicted.cpu(), average=\"micro\")\n",
        "        train_accurs.append(accuracy)\n",
        "        train_f1s.append(micro_f1)\n",
        "        train_loader_tqdm.set_postfix(loss=loss.item(), accuracy=accuracy, micro_f1=micro_f1)\n",
        "\n",
        "    train_accuracy = sum(train_accurs) / len(train_accurs)\n",
        "    train_micro_f1 = sum(train_f1s) / len(train_f1s)\n",
        "    print(\"Training - Accuracy: {:.4f}, Micro F1: {:.4f}\".format(train_accuracy, train_micro_f1))\n",
        "\n",
        "    # Validation\n",
        "    downstream_model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loader_tqdm = tqdm(val_loader, desc=\"Validation\")\n",
        "        for data, label, length in val_loader_tqdm:\n",
        "            data = data.to(device)\n",
        "            data_reverse = data.flip(1)\n",
        "            _, (xf0, xf1, xf2) = forward_lstm(data)\n",
        "            _, (xb0, xb1, xb2) = backward_lstm(data_reverse)\n",
        "            xf0, xf1, xf2 = xf0.detach(), xf1.detach(), xf2.detach()\n",
        "            xb0, xb1, xb2 = xb0.detach(), xb1.detach(), xb2.detach()\n",
        "            x0 = torch.cat((xf0, xb0), dim=2)\n",
        "            x1 = torch.cat((xf1, xb1), dim=2)\n",
        "            x2 = torch.cat((xf2, xb2), dim=2)\n",
        "            output = downstream_model(x0, x1, x2, length)\n",
        "\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            accuracy = accuracy_score(label.cpu(), predicted.cpu())\n",
        "            micro_f1 = f1_score(label.cpu(), predicted.cpu(), average=\"micro\")\n",
        "            val_accurs.append(accuracy)\n",
        "            val_f1s.append(micro_f1)\n",
        "            val_loader_tqdm.set_postfix(accuracy=accuracy, micro_f1=micro_f1)\n",
        "\n",
        "    val_accuracy = sum(val_accurs) / len(val_accurs)\n",
        "    val_micro_f1 = sum(val_f1s) / len(val_f1s)\n",
        "    print(\"Validation - Accuracy: {:.4f}, Micro F1: {:.4f}\".format(val_accuracy, val_micro_f1))\n",
        "\n",
        "    # Check if this is the best validation F1 score so far\n",
        "    if val_micro_f1 > best_val_f1:\n",
        "        best_val_f1 = val_micro_f1\n",
        "        # Save the model\n",
        "        torch.save(downstream_model, \"best_downstream_model_frozen.pt\")\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Testing the best model\n",
        "best_downstream_model = torch.load(\"best_downstream_model_frozen.pt\")\n",
        "best_downstream_model.eval()\n",
        "\n",
        "test_accurs = []\n",
        "test_f1s = []\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, label, length in test_loader:\n",
        "        data = data.to(device)\n",
        "        data_reverse = data.flip(1)\n",
        "        _, (xf0, xf1, xf2) = forward_lstm(data)\n",
        "        _, (xb0, xb1, xb2) = backward_lstm(data_reverse)\n",
        "        xf0, xf1, xf2 = xf0.detach(), xf1.detach(), xf2.detach()\n",
        "        xb0, xb1, xb2 = xb0.detach(), xb1.detach(), xb2.detach()\n",
        "        x0 = torch.cat((xf0, xb0), dim=2)\n",
        "        x1 = torch.cat((xf1, xb1), dim=2)\n",
        "        x2 = torch.cat((xf2, xb2), dim=2)\n",
        "        output = best_downstream_model(x0, x1, x2, length)\n",
        "\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        true_labels.extend(label.cpu().numpy())\n",
        "        predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "        accuracy = accuracy_score(label.cpu(), predicted.cpu())\n",
        "        micro_f1 = f1_score(label.cpu(), predicted.cpu(), average=\"micro\")\n",
        "        test_accurs.append(accuracy)\n",
        "        test_f1s.append(micro_f1)\n",
        "\n",
        "test_accuracy = sum(test_accurs) / len(test_accurs)\n",
        "test_micro_f1 = sum(test_f1s) / len(test_f1s)\n",
        "print(\"Test - Accuracy: {:.4f}, Micro F1: {:.4f}\".format(test_accuracy, test_micro_f1))\n",
        "\n",
        "# Print confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-25T07:58:00.091898Z",
          "iopub.execute_input": "2024-04-25T07:58:00.092293Z",
          "iopub.status.idle": "2024-04-25T08:58:17.955717Z",
          "shell.execute_reply.started": "2024-04-25T07:58:00.092261Z",
          "shell.execute_reply": "2024-04-25T08:58:17.954520Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIp2Jj870-Kg",
        "outputId": "48d48f69-41d0-4bac-d4f6-817088235f11"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done importing...\n",
            "train, test input shapes:  torch.Size([20000, 117]) torch.Size([7600, 104])\n",
            "train, test label shapes:  torch.Size([20000]) torch.Size([7600])\n",
            "train, test length shapes:  torch.Size([20000]) torch.Size([7600])\n",
            "Epoch 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.46it/s, accuracy=0.4, loss=1.22, micro_f1=0.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.3690, Micro F1: 0.3690\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.51it/s, accuracy=0.48, micro_f1=0.48]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.4493, Micro F1: 0.4493\n",
            "Epoch 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:57<00:00,  5.55it/s, accuracy=0.46, loss=1.06, micro_f1=0.46]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.5026, Micro F1: 0.5026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.41it/s, accuracy=0.54, micro_f1=0.54]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.5222, Micro F1: 0.5222\n",
            "Epoch 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:57<00:00,  5.55it/s, accuracy=0.58, loss=0.952, micro_f1=0.58]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.5851, Micro F1: 0.5851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.37it/s, accuracy=0.7, micro_f1=0.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.5853, Micro F1: 0.5853\n",
            "Epoch 4:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:57<00:00,  5.58it/s, accuracy=0.68, loss=0.8, micro_f1=0.68]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.6296, Micro F1: 0.6296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.38it/s, accuracy=0.64, micro_f1=0.64]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6082, Micro F1: 0.6082\n",
            "Epoch 5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:57<00:00,  5.56it/s, accuracy=0.6, loss=0.949, micro_f1=0.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.6597, Micro F1: 0.6597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.40it/s, accuracy=0.64, micro_f1=0.64]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.5940, Micro F1: 0.5940\n",
            "Epoch 6:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:57<00:00,  5.58it/s, accuracy=0.6, loss=0.903, micro_f1=0.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.6796, Micro F1: 0.6796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.39it/s, accuracy=0.68, micro_f1=0.68]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6552, Micro F1: 0.6552\n",
            "Epoch 7:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:57<00:00,  5.57it/s, accuracy=0.72, loss=0.701, micro_f1=0.72]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.7009, Micro F1: 0.7009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.40it/s, accuracy=0.64, micro_f1=0.64]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6660, Micro F1: 0.6660\n",
            "Epoch 8:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:57<00:00,  5.57it/s, accuracy=0.6, loss=0.87, micro_f1=0.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.7142, Micro F1: 0.7142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.40it/s, accuracy=0.7, micro_f1=0.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6737, Micro F1: 0.6737\n",
            "Epoch 9:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:57<00:00,  5.56it/s, accuracy=0.86, loss=0.521, micro_f1=0.86]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.7334, Micro F1: 0.7334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.40it/s, accuracy=0.64, micro_f1=0.64]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6762, Micro F1: 0.6762\n",
            "Epoch 10:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:57<00:00,  5.58it/s, accuracy=0.7, loss=0.584, micro_f1=0.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.7470, Micro F1: 0.7470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.41it/s, accuracy=0.54, micro_f1=0.54]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6660, Micro F1: 0.6660\n",
            "Training complete.\n",
            "Test - Accuracy: 0.6353, Micro F1: 0.6353\n",
            "Confusion Matrix:\n",
            "[[1130  324  158  288]\n",
            " [ 170 1382  116  232]\n",
            " [ 189  180 1055  476]\n",
            " [ 169  210  260 1261]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learnable Function"
      ],
      "metadata": {
        "id": "dKQX-cSL0-Kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "import gensim\n",
        "import os\n",
        "\n",
        "from torch import nn, optim\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import regex as re\n",
        "print(\"Done importing...\")\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'[0-9]+', '', text)\n",
        "    text = text.strip()\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "class lstm(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(lstm, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        # lstm with 2 stacks\n",
        "        num_stacks = 2\n",
        "        self.embeddings = nn.Embedding(vocab_size, 300)\n",
        "        self.lstm = nn.LSTM(300, 300, 1, batch_first=True)\n",
        "        self.lstm1 = nn.LSTM(300, 300, 1, batch_first=True)\n",
        "        self.linear = nn.Linear(300, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed = self.embeddings(x)\n",
        "        x1, _ = self.lstm(embed)\n",
        "        x2, _ = self.lstm1(x1)\n",
        "        x = self.linear(x2)\n",
        "        return x, (embed, x1, x2)\n",
        "\n",
        "def make_dataset(data, word2idx, idx2word, max_sentences=20000):\n",
        "    seq_list = []\n",
        "    label_list = []\n",
        "    length_list = []\n",
        "    data = data[1:max_sentences+1]\n",
        "    for item in data:\n",
        "        idx_seq = []\n",
        "        item = item.strip().split(',')\n",
        "        label = item[0]\n",
        "        seq = preprocess(item[1])\n",
        "        seq = seq.split()\n",
        "        for word in seq:\n",
        "            if word in word2idx.keys():\n",
        "                idx_seq.append(word2idx[word])\n",
        "            else:\n",
        "                idx_seq.append(word2idx[\"unk\"])\n",
        "        length = len(idx_seq)\n",
        "        length_list.append(length)\n",
        "        seq_list.append(idx_seq)\n",
        "        label_list.append(label)\n",
        "    return seq_list, label_list, length_list\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "word2idx_path = \"word2idx.pt\"\n",
        "idx2word_path = \"idx2word.pt\"\n",
        "\n",
        "word2idx = torch.load(word2idx_path)\n",
        "idx2word = torch.load(idx2word_path)\n",
        "\n",
        "def load_dataset(filename):\n",
        "    file_path = os.path.join(\"drive/MyDrive/NLP_A4\", filename)\n",
        "    with open(file_path, \"r\") as file:\n",
        "        data = file.readlines()\n",
        "    return data\n",
        "\n",
        "train_data = load_dataset(\"train.csv\")\n",
        "test_data = load_dataset(\"test.csv\")\n",
        "\n",
        "#preprocess and create sequences, labels, and lengths\n",
        "train_seqs, train_labels, train_lengths = make_dataset(train_data, word2idx, idx2word)\n",
        "test_seqs, test_labels, test_lengths = make_dataset(test_data, word2idx, idx2word)\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "train_seqs = pad_sequence([torch.LongTensor(i) for i in train_seqs], batch_first=True)\n",
        "test_seqs = pad_sequence([torch.LongTensor(i) for i in test_seqs], batch_first=True)\n",
        "\n",
        "train_labels = torch.LongTensor([int(i) for i in train_labels])\n",
        "test_labels = torch.LongTensor([int(i) for i in test_labels])\n",
        "train_lengths = torch.LongTensor(train_lengths)\n",
        "test_lengths = torch.LongTensor(test_lengths)\n",
        "\n",
        "print(\"train, test input shapes: \", train_seqs.shape, test_seqs.shape)\n",
        "print(\"train, test label shapes: \", train_labels.shape, test_labels.shape)\n",
        "print(\"train, test length shapes: \", train_lengths.shape, test_lengths.shape)\n",
        "\n",
        "\n",
        "class CombinationFunction(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(CombinationFunction, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, e0, e1, e2):\n",
        "        combined = torch.cat((e0, e1, e2), dim=2)\n",
        "        output = F.relu(self.linear(combined))\n",
        "        return output\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, num_classes, in_dim):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.combination_function = CombinationFunction(300,600)\n",
        "        self.linear_layer = nn.Linear(in_dim, 600)\n",
        "        self.lstm = nn.LSTM(600, 300, 1, batch_first=True)\n",
        "        self.linear_layer2 = nn.Linear(300, num_classes)\n",
        "\n",
        "        # Initialize learnable lambda parameters\n",
        "        self.lambda_0 = nn.Parameter(torch.rand(1))\n",
        "        self.lambda_1 = nn.Parameter(torch.rand(1))\n",
        "        self.lambda_2 = nn.Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, x0, x1, x2, length):\n",
        "        x = self.linear_layer(self.lambda_0 * x0 + self.lambda_1 * x1 + self.lambda_2 * x2)\n",
        "        x, _ = self.lstm(x)\n",
        "\n",
        "        final_preds = []\n",
        "        loc_cnt = 0\n",
        "        for loc_len in length:\n",
        "            loc_x = x[loc_cnt, loc_len-1, :]\n",
        "            loc_out = self.linear_layer2(loc_x)\n",
        "            loc_cnt += 1\n",
        "            final_preds.append(loc_out)\n",
        "        final_preds = torch.stack(final_preds)\n",
        "        return final_preds\n",
        "\n",
        "forward_lstm = torch.load(\"backward_model_final.pt\").to(device)\n",
        "backward_lstm = torch.load(\"forward_model_final.pt\").to(device)\n",
        "\n",
        "# Freeze the ELMo model parameters\n",
        "for param in list(forward_lstm.parameters()) + list(backward_lstm.parameters()):\n",
        "    param.requires_grad = False\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_seqs, train_labels, train_lengths)\n",
        "\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(test_seqs, test_labels, test_lengths)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=50, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "in_num = 600\n",
        "num_classes = 5\n",
        "downstream_model = Classifier(num_classes, in_num).to(device)\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(downstream_model.parameters(), lr=0.0001)\n",
        "\n",
        "from tqdm import tqdm\n",
        "n_epochs = 10\n",
        "best_val_f1 = 0.0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_accurs = []\n",
        "    train_f1s = []\n",
        "    val_accurs = []\n",
        "    val_f1s = []\n",
        "\n",
        "    print(\"Epoch {}:\".format(epoch + 1))\n",
        "\n",
        "    # Training\n",
        "    downstream_model.train()\n",
        "    train_loader_tqdm = tqdm(train_loader, desc=\"Training\")\n",
        "    for data, label, length in train_loader_tqdm:\n",
        "        data = data.to(device)\n",
        "        data_reverse = data.flip(1)\n",
        "        _, (xf0, xf1, xf2) = forward_lstm(data)\n",
        "        _, (xb0, xb1, xb2) = backward_lstm(data_reverse)\n",
        "        xf0, xf1, xf2 = xf0.detach(), xf1.detach(), xf2.detach()\n",
        "        xb0, xb1, xb2 = xb0.detach(), xb1.detach(), xb2.detach()\n",
        "        x0 = torch.cat((xf0, xb0), dim=2)\n",
        "        x1 = torch.cat((xf1, xb1), dim=2)\n",
        "        x2 = torch.cat((xf2, xb2), dim=2)\n",
        "        output = downstream_model(x0, x1, x2, length)\n",
        "        loss = loss_func(output, label.to(device))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        accuracy = accuracy_score(label.cpu(), predicted.cpu())\n",
        "        micro_f1 = f1_score(label.cpu(), predicted.cpu(), average=\"micro\")\n",
        "        train_accurs.append(accuracy)\n",
        "        train_f1s.append(micro_f1)\n",
        "        train_loader_tqdm.set_postfix(loss=loss.item(), accuracy=accuracy, micro_f1=micro_f1)\n",
        "\n",
        "    train_accuracy = sum(train_accurs) / len(train_accurs)\n",
        "    train_micro_f1 = sum(train_f1s) / len(train_f1s)\n",
        "    print(\"Training - Accuracy: {:.4f}, Micro F1: {:.4f}\".format(train_accuracy, train_micro_f1))\n",
        "\n",
        "    # Validation\n",
        "    downstream_model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loader_tqdm = tqdm(val_loader, desc=\"Validation\")\n",
        "        for data, label, length in val_loader_tqdm:\n",
        "            data = data.to(device)\n",
        "            data_reverse = data.flip(1)\n",
        "            _, (xf0, xf1, xf2) = forward_lstm(data)\n",
        "            _, (xb0, xb1, xb2) = backward_lstm(data_reverse)\n",
        "            xf0, xf1, xf2 = xf0.detach(), xf1.detach(), xf2.detach()\n",
        "            xb0, xb1, xb2 = xb0.detach(), xb1.detach(), xb2.detach()\n",
        "            x0 = torch.cat((xf0, xb0), dim=2)\n",
        "            x1 = torch.cat((xf1, xb1), dim=2)\n",
        "            x2 = torch.cat((xf2, xb2), dim=2)\n",
        "            output = downstream_model(x0, x1, x2, length)\n",
        "\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            accuracy = accuracy_score(label.cpu(), predicted.cpu())\n",
        "            micro_f1 = f1_score(label.cpu(), predicted.cpu(), average=\"micro\")\n",
        "            val_accurs.append(accuracy)\n",
        "            val_f1s.append(micro_f1)\n",
        "            val_loader_tqdm.set_postfix(accuracy=accuracy, micro_f1=micro_f1)\n",
        "\n",
        "    val_accuracy = sum(val_accurs) / len(val_accurs)\n",
        "    val_micro_f1 = sum(val_f1s) / len(val_f1s)\n",
        "    print(\"Validation - Accuracy: {:.4f}, Micro F1: {:.4f}\".format(val_accuracy, val_micro_f1))\n",
        "\n",
        "    # Check if this is the best validation F1 score so far\n",
        "    if val_micro_f1 > best_val_f1:\n",
        "        best_val_f1 = val_micro_f1\n",
        "        torch.save(downstream_model, \"best_downstream_model_function.pt\")\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Testing the best model\n",
        "best_downstream_model = torch.load(\"best_downstream_model_function.pt\")\n",
        "best_downstream_model.eval()\n",
        "\n",
        "test_accurs = []\n",
        "test_f1s = []\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, label, length in test_loader:\n",
        "        data = data.to(device)\n",
        "        data_reverse = data.flip(1)\n",
        "        _, (xf0, xf1, xf2) = forward_lstm(data)\n",
        "        _, (xb0, xb1, xb2) = backward_lstm(data_reverse)\n",
        "        xf0, xf1, xf2 = xf0.detach(), xf1.detach(), xf2.detach()\n",
        "        xb0, xb1, xb2 = xb0.detach(), xb1.detach(), xb2.detach()\n",
        "        x0 = torch.cat((xf0, xb0), dim=2)\n",
        "        x1 = torch.cat((xf1, xb1), dim=2)\n",
        "        x2 = torch.cat((xf2, xb2), dim=2)\n",
        "        output = best_downstream_model(x0, x1, x2, length)\n",
        "\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        true_labels.extend(label.cpu().numpy())\n",
        "        predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "        accuracy = accuracy_score(label.cpu(), predicted.cpu())\n",
        "        micro_f1 = f1_score(label.cpu(), predicted.cpu(), average=\"micro\")\n",
        "        test_accurs.append(accuracy)\n",
        "        test_f1s.append(micro_f1)\n",
        "\n",
        "test_accuracy = sum(test_accurs) / len(test_accurs)\n",
        "test_micro_f1 = sum(test_f1s) / len(test_f1s)\n",
        "print(\"Test - Accuracy: {:.4f}, Micro F1: {:.4f}\".format(test_accuracy, test_micro_f1))\n",
        "\n",
        "# Print confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-25T09:20:58.701020Z",
          "iopub.execute_input": "2024-04-25T09:20:58.701687Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgwG4Bei0-Kh",
        "outputId": "91dac408-da1d-4ff3-8259-b49e5b9114ec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done importing...\n",
            "train, test input shapes:  torch.Size([20000, 117]) torch.Size([7600, 104])\n",
            "train, test label shapes:  torch.Size([20000]) torch.Size([7600])\n",
            "train, test length shapes:  torch.Size([20000]) torch.Size([7600])\n",
            "Epoch 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [01:00<00:00,  5.26it/s, accuracy=0.42, loss=1.24, micro_f1=0.42]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.4046, Micro F1: 0.4046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.44it/s, accuracy=0.38, micro_f1=0.38]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.4278, Micro F1: 0.4278\n",
            "Epoch 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.51it/s, accuracy=0.56, loss=1.09, micro_f1=0.56]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.5433, Micro F1: 0.5433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.37it/s, accuracy=0.6, micro_f1=0.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.5410, Micro F1: 0.5410\n",
            "Epoch 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:59<00:00,  5.36it/s, accuracy=0.64, loss=0.734, micro_f1=0.64]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.6349, Micro F1: 0.6349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.36it/s, accuracy=0.68, micro_f1=0.68]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6410, Micro F1: 0.6410\n",
            "Epoch 4:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.44it/s, accuracy=0.74, loss=0.692, micro_f1=0.74]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.6852, Micro F1: 0.6852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.39it/s, accuracy=0.54, micro_f1=0.54]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6568, Micro F1: 0.6568\n",
            "Epoch 5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.51it/s, accuracy=0.8, loss=0.615, micro_f1=0.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.7266, Micro F1: 0.7266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.39it/s, accuracy=0.74, micro_f1=0.74]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.6872, Micro F1: 0.6872\n",
            "Epoch 6:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.48it/s, accuracy=0.68, loss=0.724, micro_f1=0.68]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.7509, Micro F1: 0.7509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.41it/s, accuracy=0.76, micro_f1=0.76]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.7117, Micro F1: 0.7117\n",
            "Epoch 7:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.50it/s, accuracy=0.76, loss=0.665, micro_f1=0.76]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.7808, Micro F1: 0.7808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.39it/s, accuracy=0.7, micro_f1=0.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.7243, Micro F1: 0.7243\n",
            "Epoch 8:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.51it/s, accuracy=0.8, loss=0.433, micro_f1=0.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.8028, Micro F1: 0.8028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.38it/s, accuracy=0.62, micro_f1=0.62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.7085, Micro F1: 0.7085\n",
            "Epoch 9:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.51it/s, accuracy=0.84, loss=0.4, micro_f1=0.84]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.8241, Micro F1: 0.8241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.38it/s, accuracy=0.72, micro_f1=0.72]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.7105, Micro F1: 0.7105\n",
            "Epoch 10:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 320/320 [00:58<00:00,  5.51it/s, accuracy=0.8, loss=0.582, micro_f1=0.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Accuracy: 0.8493, Micro F1: 0.8493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 80/80 [00:12<00:00,  6.39it/s, accuracy=0.82, micro_f1=0.82]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - Accuracy: 0.7392, Micro F1: 0.7392\n",
            "Training complete.\n",
            "Test - Accuracy: 0.6958, Micro F1: 0.6958\n",
            "Confusion Matrix:\n",
            "[[1333  296  137  134]\n",
            " [ 174 1548   72  106]\n",
            " [ 165  235 1219  281]\n",
            " [ 207  244  261 1188]]\n"
          ]
        }
      ]
    }
  ]
}